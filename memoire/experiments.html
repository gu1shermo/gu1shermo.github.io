<h1
id="expérimentations-vers-une-interaction-avec-la-musique">Expérimentations:
vers une interaction avec la musique</h1>
<h2 id="introduction">Introduction</h2>
<p>Ce chapitre sera consacré à mes expérimentations antérieures visant à
explorer l’interaction entre la musique et les <em>shaders</em> en temps
réel. Mon choix s’est orienté vers des outils permettant de générer un
flux MIDI afin d’animer mes <em>shaders</em> en fonction des hauteurs
des notes. Dans cette optique, j’ai entrepris une étude approfondie de
la structure MIDI afin de mieux appréhender et localiser les
informations clés telles que la hauteur de la note, sa vélocité et sa
durée (une section sera dédiée à cette exploration).</p>
<p>Il existe de nombreux logiciels de <em>livecoding</em> musical parmi
lesquels choisir. Dans cette multitude, j’ai sélectionné Orca (voir <a
href="#orca00" data-reference-type="ref"
data-reference="orca00">[orca00]</a>) pour sa syntaxe ésotérique et
FoxDot (voir <a href="#foxdot00" data-reference-type="ref"
data-reference="foxdot00">1.1</a>) pour son intégration avec Python
ainsi que pour sa syntaxe intuitive proche de celle d’une partition
papier.</p>
<figure id="foxdot00">
<img src="images/experiments/orca00.jpg" />
<img src="images/experiments/foxdot00.jpg" />
<figcaption>FoxDot</figcaption>
</figure>
<p>J’approfondirai ces deux solutions plus en détail dans la suite de ce
mémoire. Il est à noter que parmi les <em>demosceners</em>, chacun a ses
propres préférences en matière de logiciel, souvent influencées par des
affinités personnelles ou par leur expérience professionnelle ou
artistique. Parmi les logiciels les plus appréciés figurent TidalCycles,
Sonic Pi, SuperCollider (qui demeure la référence en matière de synthèse
sonore) ainsi que Max/MSP (voir <a href="#tidal00"
data-reference-type="ref" data-reference="tidal00">[tidal00]</a>, <a
href="#sonicpi00" data-reference-type="ref"
data-reference="sonicpi00">1.2</a>, <a href="#sc00"
data-reference-type="ref" data-reference="sc00">[sc00]</a> et <a
href="#maxmsp00" data-reference-type="ref"
data-reference="maxmsp00">1.3</a>).</p>
<figure id="sonicpi00">
<img src="images/experiments/tidal00.png" />
<img src="images/experiments/sonicpi00.jpg" />
<figcaption>Sonic Pi</figcaption>
</figure>
<figure id="maxmsp00">
<img src="images/experiments/supercollider00.jpg" />
<img src="images/experiments/maxmsp00.jpg" />
<figcaption>MAX/MSP</figcaption>
</figure>
<p>Au cours de mon intensif de M1, j’ai élaboré le <em>pipeline</em>
suivant : d’abord générer le MIDI avec Orca, puis transférer vers
Ableton via loopMIDI<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>, puis intégrer le MIDI dans
TouchDesigner à l’aide d’un <em>plugin</em> pour finalement animer les
visuels grâce aux <em>nodes</em> de TouchDesigner. J’ai été agréablement
surpris de ma facilité à maîtriser Orca, malgré sa réputation de langage
difficile à appréhender. Cependant, je restais insatisfait, les étapes
du processus manquant de réactivité pour une utilisation en conditions
réelles lors de performances sur scène.</p>
<p>C’est pourquoi, lors de mon intensif de M2, j’ai choisi une approche
légèrement revisitée en concevant un <em>pipeline</em> à la fois
similaire et unique. Cette fois-ci, le MIDI était généré par FoxDot et
ensuite acheminé vers TouchDesigner via loopMIDI. Au sein de cette
configuration, le MIDI anime des variables uniformes intégrées au code
GLSL de TouchDesigner, qui propose un <em>node</em> dédié. Cette
démarche m’a offert l’occasion d’approfondir mes compétences avec FoxDot
et de constater la facilité avec laquelle on peut traduire un
<em>shader</em> dans TouchDesigner. Bien que cette approche m’ait paru
plus efficace que la précédente, mon objectif demeure de réduire au
maximum l’usage de logiciels tiers pour la création d’animations de
<em>shaders</em>, un objectif que j’espère atteindre à la fin de mon
mémoire.</p>
<p>Je commencerai par une analyse de mes intensifs de M1 et M2 pour
exposer ma méthodologie, ainsi que les avantages et inconvénients
rencontrés au cours de ces expérimentations pour ensuite proposer une
solution basée sur l’éditeur de <em>shaders</em> KodeLife renommé pour
sa capacité à traiter divers types d’entrées, notamment le MIDI sous
forme de texture.</p>
<h2 id="intensifs-m1">Intensifs M1</h2>
<p>Lors de mon intensif en M1, j’ai eu recours à Orca pour la
transmission de signaux MIDI (voir <a href="#orca01"
data-reference-type="ref" data-reference="orca01">1.4</a>). Orca est un
langage de programmation ésotérique<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a> (parfois également
désigné par le terme « exotique »), conçu par Hundred Rabbits<a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a> et particulièrement adapté au
<em>livecoding</em>. Le code peut être modifié en temps réel, et en
utilisant les signaux MIDI, il est possible de contrôler des éléments
visuels ou des dispositifs d’éclairage.</p>
<figure id="orca01">
<img src="images/experiments/orca01.jpg" />
<figcaption>Orca couplé avec Pilot qui fournit une liste d’instruments
MIDI</figcaption>
</figure>
<h3 class="unnumbered" id="présentation-dorca">Présentation d’Orca</h3>
<p>Orca est structuré comme une grille bidimensionnelle dans laquelle
chaque cellule peut contenir un caractère. Une cellule vide est
symbolisée par le caractère « ».</p>
<p>Les opérateurs d’Orca comprennent toutes les lettres de l’alphabet
ainsi que quelques symboles. Chaque opérateur occupe une case unique
dans la grille. La majorité des opérateurs ont une ou plusieurs entrées
à l’Est et à l’Ouest, et une sortie généralement dirigée vers le Sud<a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>. Bien que certains opérateurs soient
mobiles, la plupart demeurent fixes.</p>
<p>Chaque lettre de l’alphabet, qu’il s’agisse des majuscules (<span
class="math inline">\(A-Z\)</span>) ou des minuscules (<span
class="math inline">\(a-z\)</span>), représente un opérateur spécifique.
Les opérateurs en majuscules s’exécutent à chaque <em>frame</em>, tandis
que les opérateurs en minuscules s’exécutent à chaque <em>bang</em>,
symbolisé par le caractère « ».</p>
<p>Par exemple, l’opérateur « » génère un <em>bang</em> régulier au Sud.
Tout opérateur adjacent à cette cellule spécifique sera alors activé.
Parmi ces opérateurs, on trouve des fonctions d’addition, de
soustraction, ainsi que des fonctions pour lire et écrire dans des
variables.</p>
<p>Prenons l’exemple de l’opérateur « ». Il dispose de deux entrées («
<span class="math inline">\(8\)</span> » et « <span
class="math inline">\(2\)</span> ») et d’une sortie (« »). Ainsi, un
<em>bang</em> sera généré toutes les 16 <em>frames</em> et activera les
opérateurs adjacents.</p>
<div class="sourceCode" id="cb1" data-language="GLSL"
data-caption="Opérateur « \custominline{D} »" data-captionpos="b"
data-frame="single"><pre class="sourceCode GLSL"><code class="sourceCode glsl"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span>D2</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">.*.</span></span></code></pre></div>
<p>D’autres opérateurs ont un nombre différent d’entrées et de sorties.
Prenons l’exemple de l’opérateur « » : il dispose de trois entrées (deux
à l’Ouest correspondant au décalage horizontal et vertical et une à
l’Est correspondant au caractère que l’on souhaite écrire) et d’une
sortie au Sud. Cet opérateur permet d’écrire un caractère avec un
décalage dans la grille. Dans cet exemple, il écrit un <span
class="math inline">\(7\)</span> décalé d’une cellule vers le Sud et
d’une cellule vers l’Est.</p>
<div class="sourceCode" id="cb2" data-language="GLSL"
data-caption="Opérateur « \custominline{X} »" data-captionpos="b"
data-frame="single"><pre class="sourceCode GLSL"><code class="sourceCode glsl"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span>X7</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">....</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="op">...</span><span class="fu">7</span></span></code></pre></div>
<p>Pour jouer une note MIDI, on combinera les opérateurs « » (qui génère
un <em>bang</em> régulier) et « » (qui envoie un message MIDI). Bien que
« » n’ait pas de sortie dans Orca, il envoie un message MIDI à un
appareil MIDI.</p>
<div class="sourceCode" id="cb3" data-language="GLSL"
data-caption="Jouer une note MIDI" data-captionpos="b"
data-frame="single"><pre class="sourceCode GLSL"><code class="sourceCode glsl"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>D8<span class="op">.....</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">*:</span>A2F51</span></code></pre></div>
<p>L’opérateur « » attend 5 arguments mais seuls les trois premiers sont
obligatoires.</p>
<ul>
<li><p><strong>A (argument 1)</strong>: le channel MIDI sur lequel
envoyer le message (A channel 10)</p></li>
<li><p><strong>2 (argument 2)</strong>: l’octave (2 Troisième
octave)</p></li>
<li><p><strong>F (argument 3)</strong>: la note (F Fa)</p></li>
<li><p><strong>5 (argument 4)</strong>: la vélocité (la force avec
laquelle la touche a été frappée lorsque la note a été jouée)</p></li>
<li><p><strong>1 (argument 5)</strong>: la durée (le temps pendant
lequel la note est tenue)</p></li>
</ul>
<p>C’est le point de départ pour générer un son, mais en associant
divers opérateurs, on peut produire une gamme de possibilités, telles
que jouer des notes de façon aléatoire, stocker des mélodies dans des
tableaux, créer et exécuter des séquences mélodiques, utiliser des
variables, et appliquer la logique booléenne pour établir des
conditions, entre autres. Le défi réside dans la quête d’une harmonie
musicale tout en concevant un visuel singulier. De manière personnelle,
je trouve aussi un intérêt particulier à explorer l’aspect visuel du
langage, comme en recréant un Tetris synchronisé avec la musique de
Tetris (voir <a href="#tetris00" data-reference-type="ref"
data-reference="tetris00">[tetris00]</a>). Le lecteur pourra retrouver
la liste de tous les opérateurs en annexe <a href="#appendix:orcaops"
data-reference-type="ref"
data-reference="appendix:orcaops">[appendix:orcaops]</a>.</p>
<h3 class="unnumbered" id="pipeline-m1"><em>Pipeline</em> M1</h3>
<p>Lors de mon intensif, j’ai acheminé les signaux MIDI d’Orca vers
Ableton, puis d’Ableton vers TouchDesigner via un réseau nodal pour
interpréter les valeurs MIDI et animer les visuels (voir <a
href="#orcatouch00" data-reference-type="ref"
data-reference="orcatouch00">1.6</a> et <a href="#orcatouch01"
data-reference-type="ref" data-reference="orcatouch01">1.7</a>). Bien
que cette méthode soit efficace, elle ne m’a pas semblé suffisamment
proche de la pratique du <em>livecoding</em>. C’est lors de mon intensif
de M2 que j’ai pu explorer une approche plus directe, en exploitant le
<em>node</em> de TouchDesigner spécifiquement conçu pour le <em>fragment
shader</em>.</p>
<figure id="orca_00">
<img src="images/experiments/tetris00.png" />
<img src="images/experiments/orca_ableton00.png" />
<figcaption>Orca vers Ableton</figcaption>
</figure>
<figure id="orcatouch00">
<img src="images/experiments/orcatouch00.png" />
<img src="images/experiments/orcatouch01.png" />
<figcaption>Animation de visuels 3D avec Orca</figcaption>
</figure>
<figure id="orcatouch01">
<img src="images/experiments/orcatouch02.png" />
<img src="images/experiments/orcatouch03.png" />
<figcaption>Animation de visuels 2D avec Orca</figcaption>
</figure>
<h2 id="intensifs-m2">Intensifs M2</h2>
<h3 class="unnumbered" id="contexte-et-objectif">Contexte et
objectif</h3>
<p>Suite à ma déception concernant le manque d’interactivité en temps
réel après l’intensif de M1, j’ai décidé, dans le cadre de mon intensif
de M2, d’explorer l’utilisation de FoxDot pour la création de flux MIDI.
L’objectif initial de cette démarche était de concevoir une performance
de <em>livecoding</em> qui mettrait en évidence l’interaction entre le
son produit via le code de FoxDot et les <em>fragment shaders</em> (voir
<a href="#intensifs00" data-reference-type="ref"
data-reference="intensifs00">1.8</a>).</p>
<figure id="intensifs00">
<img src="images/experiments/intensifs/intensifs00.png" />
<img src="images/experiments/intensifs/intensifs01.png" />
<figcaption>Ébauches de <em>pipeline</em> pour l’intensif de
M2</figcaption>
</figure>
<p>Dans cette section, je vais présenter ma démarche et évoquer les
difficultés que j’ai rencontrées durant ce projet. Mais avant cela
j’aimerais approfondir l’étude de FoxDot comme j’ai pu le faire avec
Orca.</p>
<figure id="intensifs21">
<img src="images/experiments/intensifs/intensifs21.png" />
<img src="images/experiments/intensifs/intensifs22.png" />
<figcaption> Interactions entre FoxDot et TouchDesigner via le
GLSL<br />
(le code est à peine visible mais il est bien présent) </figcaption>
</figure>
<h3 class="unnumbered" id="foxdot-documentation">FoxDot
documentation</h3>
<p>FoxDot est un environnement de programmation Python qui fonctionne en
tandem avec SuperCollider, un langage de programmation dédié à la
synthèse audio en temps réel et au traitement du signal. Bien que je
n’aborde pas en détail les capacités de SuperCollider dans ce mémoire,
il est important de souligner sa puissance et sa capacité à créer des
sons électroniques et des effets sonores complexes à partir de zéro
grâce à la synthèse sonore. Néanmoins, l’interaction avec SuperCollider
peut être simplifiée grâce à FoxDot. On utilise SuperCollider en
arrière-plan pour configurer les instruments, puis on redirige le flux
audio vers des DAWs<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a> (<em>Digital Audio Workstation</em>)
comme Reaper ou Ableton pour écouter les instruments.</p>
<figure id="reaper00">
<img src="images/experiments/reaper00.PNG" />
<figcaption>Reaper couplé avec Vital pour la synthèse
sonore</figcaption>
</figure>
<p>Ce qui m’a particulièrement attiré vers FoxDot, malgré sa
documentation souvent incomplète et dispersée sur le web, c’est sa
similitude avec l’écriture d’une partition musicale et sa syntaxe objet
propre au langage Python. Fort de mes connaissances en solfège acquises
par ma pratique du piano et du bandonéon, j’y vois une analogie avec la
représentation d’une partition traditionnelle. Une mélodie est
symbolisée par des listes, qui peuvent elles-mêmes être considérées
comme des objets. Ainsi, on peut accéder à de nombreux attributs tels
que la hauteur de la note, sa durée, le <em>sustain</em>, et bien
d’autres.</p>
<h4 class="unnumbered" id="explication-dune-ligne-de-code">Explication
d’une ligne de code</h4>
<p>Pour mieux comprendre la logique propre à FoxDot nous allons analyser
pas à pas une ligne de code « classique ».</p>
<pre data-caption="Exemple de base FoxDot" data-captionpos="b"
data-frame="single"><code>p1 &gt;&gt; bass([0,1,2,3,4,5], dur=PDur(3,8), amp=[1,1/2,1/2]).every(6,&quot;stutter&quot;,4,dur=3,oct=6)</code></pre>
<p>Dans cet exemple, nous associons un objet à la variable à l’aide de
l’opérateur . Le terme qui suit immédiatement, , désigne l’instrument
qui sera utilisé dans SuperCollider. Nous définissons ensuite les
arguments du . Le premier argument, qui attend une liste, spécifie les
notes à jouer. Ainsi, correspond aux notes Do, Ré, Mi, Fa, Sol et La
dans le contexte de la gamme de Do majeur.</p>
<p>Les attributs suivants, et , déterminent respectivement la durée des
notes et l’amplitude du son. La partie finale avec la méthode applique
la méthode tous les 6 temps, avec une durée de 3 et une octave de 6 pour
chaque note.</p>
<h4 class="unnumbered" id="les-variables-temporelles">Les variables
temporelles</h4>
<p>Une caractéristique unique de FoxDot est la présence de variables qui
évoluent dans le temps. Lors de la programmation musicale en direct, il
est souvent souhaitable que les éléments évoluent progressivement. Cela
peut être illustré par l’utilisation de séquences d’accords. Par
exemple, pour représenter la séquence d’accords Do-Fa-Do-Sol, on
pourrait utiliser la liste Python et souhaiter jouer chaque accord
pendant 8 temps. Pour cela, on peut utiliser des variables qui varient
dans le temps, de manière à changer d’accord après 8 temps. Ces
variables sont désignées sous le nom de « variables dépendantes du temps
» ou en code.</p>
<p>Pour résoudre notre problème initial de lecture d’une séquence
d’accords, nous pourrions utiliser le code suivant, en ajustant la durée
selon nos besoins, pour voir la séquence jouer sur 8 temps pour chaque
accord :</p>
<div class="sourceCode" id="cb5" data-language="Python"
data-caption="Variables temporelles" data-captionpos="b"
data-frame="single"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">&gt;&gt;</span> pluck(var([<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">4</span>], <span class="dv">8</span>), dur<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">4</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">4</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>]) <span class="op">+</span> (<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span></code></pre></div>
<h4 class="unnumbered" id="créer-ses-propres-fonctions">Créer ses
propres fonctions</h4>
<p>La syntaxe de FoxDot offre une grande flexibilité et précision dans
la création de motifs musicaux, tout en restant accessible pour ceux qui
sont familiarisés avec la musique et la programmation.</p>
<p>Un autre avantage considérable pour ceux qui maîtrisent la
programmation est la possibilité de créer ses propres fonctions. Écrire
des fonctions personnalisées dans FoxDot permet d’élargir
significativement les capacités du langage, permettant ainsi de
concevoir des motifs musicaux plus complexes et adaptés à des besoins
spécifiques.</p>
<p>La fonction suivante s’ajoute aux méthodes disponibles pour les
objets grâce à un décorateur. Le décorateur est utilisé pour créer des
méthodes pouvant être appelées par tous les objets . Cela offre la
possibilité d’intégrer de nouvelles fonctionnalités ou de modifier le
comportement des objets sans nécessiter de modifications du code source
de FoxDot lui-même. Cette fonction a pour objectif de gérer le <em>fade
in</em><a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a> et le <em>fade out</em><a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> afin de contrôler l’apparition et la
disparition des instruments.</p>
<p>À l’intérieur du corps de la fonction, il est possible d’incorporer
tout code FoxDot valide pour générer ou manipuler le son. Cela peut
englober la création de , la définition de séquences de notes,
l’application d’effets sonores, entre autres.</p>
<div class="sourceCode" id="cb6" data-language="Python"
data-caption="Écrire ses propres fonctions" data-captionpos="b"
data-frame="single"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="at">@player_method</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fade(<span class="va">self</span>, dur<span class="op">=</span><span class="dv">8</span>, fvol<span class="op">=</span><span class="dv">1</span>, ivol<span class="op">=</span><span class="va">None</span>, autostop<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ivol <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        ivol <span class="op">=</span> <span class="bu">float</span>(<span class="va">self</span>.amplify)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.amplify <span class="op">=</span> linvar([ivol, fvol], [dur, inf], start<span class="op">=</span><span class="va">self</span>.metro.mod(<span class="dv">4</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> static_final_value():</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> fvol <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> autostop:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.stop()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.amplify <span class="op">=</span> fvol</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.metro.schedule(static_final_value, <span class="va">self</span>.metro.next_bar()<span class="op">+</span>dur<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span></span></code></pre></div>
<h3 class="unnumbered" id="envoi-du-midi">Envoi du MIDI</h3>
<p>Depuis FoxDot, un Player dédié permet l’envoi de messages MIDI vers
SuperCollider via la commande . Ce signal MIDI peut ensuite être dirigé
vers loopMIDI, un utilitaire de routage MIDI, à l’aide de la commande
dans SuperCollider (voir <a href="#intensifs03"
data-reference-type="ref" data-reference="intensifs03">[intensifs03]</a>
et <a href="#loopmidi00" data-reference-type="ref"
data-reference="loopmidi00">1.11</a>).</p>
<figure id="loopmidi00">
<img src="images/experiments/intensifs/intensifs03.png" />
<img src="images/experiments/loopmidi00.PNG" />
<figcaption>Redirection dans loopMIDI</figcaption>
</figure>
<p>Dans TouchDesigner, le flux MIDI est récupéré via le gestionnaire de
périphériques grâce au <em>node</em> « midiIn ». Étant donné que le
signal reçu est normalisé, il est alors possible de redéfinir les
intervalles aisément en utilisant le <em>node</em> « Math » pour animer
n’importe quelle variable dans TouchDesigner, notamment les variables
uniformes des <em>nodes</em> GLSL (voir <a href="#intensifs19"
data-reference-type="ref" data-reference="intensifs19">[intensifs19]</a>
et <a href="#intensifs20" data-reference-type="ref"
data-reference="intensifs20">1.12</a>).</p>
<figure id="intensifs20">
<img src="images/experiments/intensifs/intensifs19.png" />
<img src="images/experiments/intensifs/intensifs20.png" />
<figcaption>Redirection du MIDI</figcaption>
</figure>
<h3 class="unnumbered"
id="traduction-du-code-glsl-vers-touchdesigner">Traduction du code GLSL
vers TouchDesigner</h3>
<p>L’une de mes préoccupations principales était la conversion du code
GLSL vers la syntaxe attendue par les <em>nodes</em> de TouchDesigner.
Comme mentionné plus tôt dans le mémoire, cette transition entre les
langages <em>shaders</em> peut présenter certaines subtilités.</p>
<p>Initialement, je craignais que cette étape de traduction ne rencontre
des obstacles imprévus. Toutefois, cette appréhension s’est avérée
largement infondée, du moins en grande partie. Le processus de
conversion requiert quelques ajustements, dont la plupart peuvent être
automatisés : il s’agit notamment des ajustements relatifs aux types ,
ou encore les qu’on écrit avec ou sans « », la transmission de la
résolution en tant que variable uniforme, ainsi que la syntaxe
appropriée lors de l’utilisation de textures.</p>
<p>Avec Loïck (mon partenaire de projet avec Jihed), nous avions même
envisagé de développer un outil spécifique intégré à TouchDesigner pour
automatiser l’intégration de <em>shaders</em> provenant de Shadertoy.
Cependant, après nos expérimentations, nous avons abandonné cette idée.
En effet, chaque <em>shader</em> importé peut comporter des
caractéristiques spécifiques qui rendent difficile leur généralisation
au sein d’un unique programme, ce qui aurait été trop ambitieux.</p>
<p>Les véritables difficultés rencontrées étaient principalement liés
aux bugs de rendu, en particulier l’adaptation du code pour utiliser le
bon espace sur lequel dessiner, notamment la transformation des uv. La
traduction vers TouchDesigner s’est donc avérée plus aisée que prévu,
nous avons donc saisi cette opportunité pour préparer le terrain pour
Jihed. Nous avons isolé et configuré des variables pouvant influencer le
<em>shader</em>. Pour chaque variable identifiée, nous avons défini une
plage de valeurs pour faciliter les animations futures.</p>
<figure id="intensifs12">
<img src="images/experiments/intensifs/intensifs05.png" />
<img src="images/experiments/intensifs/intensifs12.png" />
<figcaption>Ajout de variables uniformes</figcaption>
</figure>
<h3 class="unnumbered"
id="limitations-de-foxdot-dans-la-gestion-du-signal-midi">Limitations de
FoxDot dans la gestion du signal MIDI</h3>
<p>FoxDot aurait été idéal pour mes besoins si il ne présentait pas une
limitation significative concernant la gestion du signal MIDI.
Actuellement, il n’est pas possible de jouer un son tout en transmettant
simultanément le signal MIDI. Pour pallier cette contrainte durant mon
intensif, j’ai créé l’illusion d’une diffusion musicale en temps réel
via le <em>livecoding</em>, alors qu’en réalité, un dédié gérait en
parallèle l’émission régulière de signaux MIDI qui correspondaient au
rythme de la musique.</p>
<p>Heureusement, bien que la version officielle de FoxDot ne soit plus
mise à jour par son créateur originel, une version 2.0 (Renardo) est en
cours de développement par Élie Gavoty, un membre du <em>Cookie
Collective</em>. J’ai pu discuter directement de ce problème avec lui,
et il l’a placé parmi ses priorités.</p>
<h3 class="unnumbered" id="imprévu-technique-et-adaptation">Imprévu
technique et adaptation</h3>
<p>J’ai omis de mentionner un autre obstacle technique auquel j’ai été
confronté lors de cet intensif. Comme précédemment expliqué, mon
objectif était de récupérer un signal MIDI plutôt qu’un signal
analogique, afin de m’éloigner des interactions traditionnellement
associées à l’interaction image-son. Au lieu d’une analyse fréquentielle
(aigus, basses, tempo, etc.), je souhaitais naïvement récupérer la
hauteur de la note dans TouchDesigner, en anticipant l’existence d’un
<em>node</em> approprié à cet effet.</p>
<p>Cependant, ce que TouchDesigner reçoit est bien la hauteur de la
note, mais sous forme de nom de variable contenant la valeur de
l’amplitude de la note jouée (voir <a href="#intensifs04"
data-reference-type="ref" data-reference="intensifs04">1.14</a>). Par
conséquent, pour récupérer la véritable hauteur de la note, il aurait
été nécessaire de développer un <em>node</em> personnalisé en Python
capable d’analyser la chaîne de caractères du nom de la variable et
d’extraire le suffixe numérique indiquant la hauteur de la note. N’ayant
identifié ce problème que tardivement, je n’ai pas eu l’opportunité de
développer ce <em>node</em> spécifique (peut-être lors d’une future
tentative).</p>
<p>Ainsi, le signal MIDI récupéré dans TouchDesigner est en quelque
sorte incomplet : bien que le rythme musical soit respecté, la hauteur
des notes n’a aucune véritable influence sur l’interaction.</p>
<figure id="intensifs04">
<img src="images/experiments/intensifs/intensifs04.png" />
<img src="images/experiments/intensifs/intensifs19.png" />
<figcaption>Problème de récupération de la hauteur de la
note</figcaption>
</figure>
<h2 id="le-projet-du-mémoire">Le projet du mémoire</h2>
<p>Pour ce mémoire, j’ai opté pour l’exploration d’un autre
<em>pipeline</em> de traitement. Initialement, mon intention était de
réaliser cette expérimentation avec OpenGL<a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, en
utilisant du code graphique bas niveau pour bénéficier d’un contrôle
maximal. Cette approche représente le choix le plus complexe en termes
de pré-requis techniques, mais elle offre également le plus de
possibilités et de contrôle. Il est tout à fait envisageable de générer
du MIDI en C++, puis d’assurer le passage des informations du CPU vers
le GPU via des variables uniformes, et enfin d’utiliser ces informations
pour animer des <em>vertex shaders</em>, des <em>geometry shaders</em>
ou bien sûr des <em>fragment shaders</em> en GLSL.</p>
<p>Au cours de ce semestre, j’ai eu l’opportunité de suivre les
enseignements de Farès Belhadj sur la programmation graphique en tant
qu’auditeur libre. J’ai saisi cette occasion pour perfectionner mes
compétences en programmation OpenGL avec pour but de participer au
concours de fin d’année visant à produire un exécutable de musique et un
<em>shader</em> d’une taille inférieure à 64 Ko.</p>
<p>Mon projet initial consistait à élaborer une fonction en C++ capable
d’interpréter et de décomposer un flux MIDI côté CPU, puis de
transmettre les attributs de chaque note au fragment <em>shader</em> via
des variables uniformes. Toutefois, mon niveau actuel en C++ et en
OpenGL n’est pas suffisamment avancé pour concevoir cette fonction à
partir de zéro. J’ai donc préféré ne pas m’y aventurer, d’autant plus
que la rédaction du mémoire représente déjà une tâche exigeante en
termes de temps et d’efforts. J’ai néanmoins examiné des solutions
permettant de composer du MIDI directement en code (voir annexe <a
href="#appendix:codemidi" data-reference-type="ref"
data-reference="appendix:codemidi">[appendix:codemidi]</a>).</p>
<p>J’ai donc opté pour KodeLife, un IDE dédié au <em>livecoding</em> qui
présente une interface très intuitive où les modifications du code sont
immédiatement visualisées à l’écran. Contrairement à Shadertoy nous
avons accès au <em>vertex shader</em> ainsi qu’au <em>geometry
shader</em>. Un autre avantage considérable est la diversité des entrées
disponibles : non seulement on peut importer des textures, mais aussi
utiliser des paramètres tels que le temps, l’image précédente du
programme, et surtout, dans notre cas particulier, l’audio. Cela peut se
faire à partir d’un fichier audio simple ou d’un flux MIDI.</p>
<figure id="kodelife01">
<img src="images/experiments/kodelife00.png" />
<img src="images/experiments/kodelife01.png" />
<figcaption><em>Input</em> du temps</figcaption>
</figure>
<figure id="kodelife03">
<img src="images/experiments/kodelife02.png" />
<img src="images/experiments/kodelife03.png" />
<figcaption><em>Input</em> de l’éditeur de Kodelife</figcaption>
</figure>
<p>Au début, j’ai été déconcerté en voyant que KodeLife représentait un
fichier MIDI sous la forme d’une image en niveaux de gris. Cependant,
grâce aux conseils de z0rg, j’ai finalement pu comprendre la
situation.</p>
<p>La principale difficulté réside dans la conceptualisation du fait
qu’un son puisse être représenté par une texture. Cette idée peut
sembler peu intuitive au départ, mais elle est en réalité très logique.
En effet, la musique est généralement représentée par une structure
MIDI, qui est essentiellement une organisation de données. Cependant,
rien n’empêche de représenter ces données sous forme d’une image, où
chaque <em>pixel</em> correspond à un échantillon du fichier MIDI.</p>
<p>En utilisant le langage de <em>shader</em> GLSL, il est ensuite
possible d’extraire les informations désirées à partir des
<em>pixels</em> de la texture, de la même manière que l’on accéderait à
n’importe quelle autre texture. Par exemple, un <em>pixel</em> aux
coordonnées <span class="math inline">\((x, y)\)</span> pourrait
contenir les informations NoteOn/NoteOff de la note LA à l’octave 1.</p>
<p>Sur la documentation officielle de KodeLife, on trouve des
indications sur l’<em>offset</em> à respecter pour obtenir les
informations souhaitées. Par exemple, pour récupérer la vélocité<a
href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a> (<em>velocity</em> en anglais) d’une
note à partir du signal , on peut utiliser le bout de code suivant :
.</p>
<p>Malheureusement, les recommandations fournies par la documentation
sont inexactes, tout comme l’exemple accessible directement dans
KodeLife. Pour résoudre ce problème, j’ai dû littéralement « compter »
les rangées et les colonnes de <em>pixels</em> à partir de flux MIDI
très minimalistes afin de trouver les bonnes valeurs. Cette tâche s’est
avérée très fastidieuse et m’a conduit à entreprendre une étude
approfondie de la structure d’un fichier MIDI afin de ne plus travailler
à l’aveugle.</p>
<h2 id="le-protocole-midi">Le protocole MIDI</h2>
<p>Le MIDI, ou <em>Musical Instrument Digital Interface</em>, est un
protocole destiné à la transmission et au stockage d’informations
relatives à la performance musicale. Ce format existe depuis près de
quarante ans (la norme est mise au point en 1982), et sa longévité est
due à son efficacité à utiliser les données numériques pour transmettre
des informations musicales.</p>
<p>Dans le domaine musical, la précision du timing est primordiale, ce
qui nécessite des communications rapides et efficaces. Pour comprendre
l’efficacité du MIDI, il est essentiel de revenir sur quelques concepts
de base tels que les bits, les octets et le système binaire. Un bit est
une unité binaire ayant deux états : soit <span
class="math inline">\(0\)</span>, soit <span
class="math inline">\(1\)</span>. Les ordinateurs comptent en binaire,
ce qui diffère du système décimal que nous utilisons habituellement. Par
exemple, en binaire, trois chiffres peuvent représenter jusqu’à huit
valeurs différentes (<span class="math inline">\(2^3\)</span>), tandis
qu’en décimal, trois chiffres peuvent représenter jusqu’à mille valeurs
(<span class="math inline">\(10^3\)</span>).</p>
<p>Dans le système binaire, le nombre de valeurs qu’un octet (un groupe
de huit bits) peut représenter est <span
class="math inline">\(2^8\)</span>, soit <span
class="math inline">\(256\)</span>. Le MIDI utilise principalement des
messages composés de deux ou trois octets. Ces octets sont classés en
deux catégories : les octets de statut et les octets de données. Les
octets de statut indiquent la nature de l’action à effectuer (par
exemple, le signal ), tandis que les octets de données fournissent les
valeurs correspondantes (dans mon cas je recherche la vélocité).</p>
<figure id="midi00">
<img src="images/experiments/midi00.jpg" style="width:12cm" />
<figcaption>Structure d’un octet de données</figcaption>
</figure>
<p>En MIDI, le premier bit d’un octet de statut est toujours un <span
class="math inline">\(1\)</span>, et le premier bit d’un octet de
données est toujours un <span class="math inline">\(0\)</span>. Ainsi,
un octet de données ne dispose que de sept bits pour coder
l’information, soit <span class="math inline">\(2^7\)</span> (128
valeurs différentes). Ce nombre, de <span
class="math inline">\(0\)</span> à <span
class="math inline">\(127\)</span>, peut couvrir la gamme des hauteurs
MIDI, des vélocités MIDI, des informations de contrôleur, ainsi que les
instruments du General MIDI (numérotés de <span
class="math inline">\(1\)</span> à <span
class="math inline">\(128\)</span>).</p>
<p>La limitation à <span class="math inline">\(128\)</span> valeurs peut
sembler restreinte, mais pour la plupart des applications musicales,
cette gamme est suffisante. En effet, cela permet de stocker une grande
quantité d’informations dans un espace réduit, ce qui est
particulièrement efficace pour le stockage et la transmission de
données.</p>
<h3 class="unnumbered" id="canal-midi">Canal MIDI</h3>
<p>La désignation du canal MIDI occupe une partie de l’octet de statut,
permettant de configurer les appareils MIDI pour qu’ils répondent
uniquement aux messages transmis sur des canaux spécifiques. Par
exemple, un synthétiseur peut être configuré pour écouter les canaux 7,
8 et 9, en ignorant les autres. Cela offre une flexibilité dans la
gestion des flux d’informations MIDI entre différents appareils. Définir
le canal MIDI dans l’octet de statut permet de cibler les informations à
recevoir par un instrument MIDI spécifique. Par exemple, chaque
instrument peut être assigné à un canal MIDI différent, permettant une
gestion précise des messages envoyés à chaque instrument.</p>
<h3 class="unnumbered" id="octet-de-statut-et-types-de-messages">Octet
de statut et types de messages</h3>
<p>Un octet de statut est composé de huit bits. Le premier bit à <span
class="math inline">\(0\)</span> indique que l’octet représente un octet
de statut. Les trois bits suivants désignent le type de message, tandis
que les quatre derniers bits représentent le canal MIDI. Avec quatre
bits pour le canal, il existe seize canaux MIDI différents. Concernant
le type de message, trois bits permettent huit types de messages MIDI.
Nous analysons en annexe <a href="#appendix:midi"
data-reference-type="ref"
data-reference="appendix:midi">[appendix:midi]</a> les sept premiers
types de messages MIDI, laissant de côté le huitième qui est spécifique
au fabricant et à l’horloge MIDI.</p>
<figure id="midi01">
<img src="images/experiments/midi01.png" style="width:12cm" />
<figcaption>Exemple du message <em>Note On</em></figcaption>
</figure>
<h3 class="unnumbered" id="conclusion">Conclusion</h3>
<p>Pour résumer, les messages MIDI représentent le langage standard
utilisé par de nombreux appareils pour communiquer entre eux. Ils se
caractérisent par une structure bien définie, où les informations sont
stockées sous forme d’octets de statut et d’octets de données. Un
message MIDI est donc formé d’un octet de statut, qui indique le type de
message et le canal de transmission, suivi d’un ou deux octets de
données fournissant des détails sur les valeurs du message. Cette
structure assure la cohérence et l’interprétation correcte des
informations transmises.</p>
<p>Il est important de souligner qu’aujourd’hui, le MIDI n’est pas
l’unique moyen de transmission de données musicales. Toutefois, il
demeure largement utilisé. De nouvelles technologies telles que l’OSC<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a> (<em>Open Sound Control</em>) et le
MIDI à haute résolution émergent, offrant ainsi des opportunités
innovantes pour la communication et un contrôle musical plus raffiné.
Cependant, cela n’indique pas une obsolescence imminente du protocole
MIDI original.</p>
<p>Ainsi, en approfondissant ma compréhension de la structure d’un
fichier MIDI, j’ai pu accéder aux données souhaitées dans KodeLife. Il a
simplement fallu attribuer chaque note d’un clavier MIDI en piochant
dans la texture MIDI, puis normaliser ces valeurs pour animer toute
variable définie dans le code GLSL.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>loopMIDI est un logiciel qui permet de créer des ports
MIDI virtuels sur un ordinateur Windows. Ces ports MIDI virtuels peuvent
être utilisés pour router des données MIDI entre différentes
applications.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Un langage ésotérique est un langage de programmation
conçu de manière intentionnellement compliquée, obscure ou non
conventionnelle. Ces langages ne sont généralement pas créés dans le but
d’être utilisés pour des applications pratiques ou professionnelles,
mais plutôt pour explorer des concepts informatiques, artistiques ou
philosophiques, ou pour le plaisir de la programmation.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Hundred Rabbits est un studio indépendant créatif
composé de Rekka Bellum et de Devine Lu Linvega, un duo de développeurs,
artistes et explorateurs vivant sur un voilier nommé Pino. Ils sont
connus pour leur approche minimaliste et expérimentale de la création,
produisant des jeux vidéo, des outils, des expériences et des œuvres
artistiques dans divers domaines tels que la programmation, la musique,
la vidéo et l’écriture.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Dans le contexte d’Orca je préfère utiliser la
terminologie Est, Sud, Ouest et Nord plutôt que droite, bas, gauche,
haut (<em>right</em>, <em>bottom</em>, <em>left</em>, <em>top</em> en
anglais)<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Une DAW est une station de travail audionumérique. C’est
un système logiciel utilisé pour l’enregistrement, l’édition, le mixage
et la production audio.<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Le <em>fade in</em> consiste en une augmentation
progressive du volume audio depuis le silence (ou un volume très bas)
jusqu’au volume souhaité. Cela crée une transition douce, permettant au
son de s’introduire de manière progressive dans l’environnement
sonore.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Le <em>fade out</em> représente une diminution
progressive du volume audio, aboutissant finalement à un silence complet
(ou à un volume très bas). Cette technique est souvent utilisée pour
conclure une piste audio ou pour introduire un changement de scène de
manière fluide.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>OpenGL (<em>Open Graphics Library</em>) est une
spécification standardisée pour le développement d’applications
graphiques interactives en 2D et 3D. Elle offre une interface de
programmation qui permet aux développeurs de créer des applications
graphiques de haute performance et de haute qualité sur différentes
plateformes matérielles. OpenGL est largement utilisé dans les domaines
tels que les jeux vidéo, la modélisation et l’animation 3D, la
visualisation scientifique et médicale, la conception assistée par
ordinateur (CAO) et bien d’autres.<a href="#fnref8"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Dans le protocole MIDI, la vélocité fait référence à la
force avec laquelle une touche est pressée sur un clavier ou un autre
contrôleur MIDI.<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>L’OSC (<em>Open Sound Control</em>) est un protocole de
communication pour les réseaux multimédias développé pour transmettre
des données de contrôle entre les logiciels et les appareils
audiovisuels. Contrairement au MIDI, qui est principalement basé sur le
format série de 8 bits, l’OSC utilise des paquets de données basés sur
des messages.<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
